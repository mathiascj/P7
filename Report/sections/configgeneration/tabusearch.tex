\section{Tabu Search}
We have defined, how we may set up configurations and orders in python, as well as how to get the rating of a configuration producing a specific order. We now tackle the problem of finding the optimal configuration, given an order of items and a set of avaliable modules. Finding the optimal configuration is a difficult problem to solve. Therefore we do not focus on creating an optimization algorithm, but rather a heuristic. Unlike optimization algorithms, heuristics do not guarentee a globally optimal solution. Instead it promises a locally optimal solution, which may be good enough in practice.

\subsection{Choosing a Metaheuristic}
A metaheuristic is a problem-independent technique used to develop heuristics for optimization problems. We choose to look into the local search family of metaheuristics. These describe heuristics, where we try to tackle the problem of optimizing some measure, by moving between candidate solutions to our problem. In our case, the measure we want to optimize is the configuration rating, where we want to find the configuration with the minimal value.

The most basic form of local search is hill climbing. Here we perform a local search by starting from some initial candidate solution. We then generate the neighbouring solutions and compare them on their measure value. The function returning the set of neighbours to a solution x is called $N(x)$. The neighbour with the best measure is chosen as the new frontier. Search continues by generating all neighbours to this solution as to compare measures and find an even better frontier. This continues until we have a frontier, where no neighbours have a better measure. The final frontier is then chosen as the best solution. The problem with this approach is that we are very likely to move into a local optima. 

Tabu Search is another type of local search, developed by Fred W. Glover \cite{glover2006}. It focuses on using memory as a manner of moving away from local optima, which otherwise catches us when hill climbing. Memory comes in two different flavors short term and long term. As we search, any solution that we pick as frontier is added to the short term memory, also known as a tabu list. Let us call the set of solutions in short term $S$. We define the set of neighbours to a frontier x, from which we may pick a new frontier as $N(x) \setminus S$. This means that we can not pick any neighbour reciding in short term memory. This allows us to pick neighbours, which we would otherwise have dismissed as frontiers. In addition, we are allowed to pick an ill fitting neighbour to be frontier, if no other neighbour optimizes our measure. Both of these constructs guide us around local optima.

Long term memory has a few different definitions. It may be used as an extension of short term memory. Both short and long term have a limited size. This means that they will have to forget old frontiers to make room for new ones. When short term memory forgets a solution, it may end up in long term memory and continue to have us steer clear of that particular solution. In other implementations, long term memory is instead used to reset search. It may happen that we enter a bad search area with many low ranking local optima. To escape this, we may replace our current frontier with one found in long term memory, as to drive search into a new area.

Memories may also be defined in different ways. A memory may describe an entire solution or perhaps just a subset of attribute values for a solution. In short term memory, the latter will in general exclude more possible frontiers. 

When performing a tabu search, there needs to be a balance between diversification and intensivation. Hill Climbing is pure intensivations, where we constantly try to optimize our measure. Random search on the other hand is pure diversification, where there is no active attempt made to optimize our measure. Depending on implementation, many factors in a tabu search may control diversification and intensivation. As an example, the larger short term memory is, the more diversification. The smaller it is the more intensivation.


\subsection{Tabu Search Implementation}
In this subsection, we will describe how we have implemented a tabu search, which is used to optimize on the configuration rating. While it has been implemented in python, we have decide to explain it using psuedo code as to easier communicate, what we have done. The psuedo code can be seen in \cref{code:psuedotabu}. The actual implementation can be found in  \textit{tabu\_search.py}. 

\lstinputlisting[float, language=Python, caption= pseudocode showing a simplified version of the tabu search implementation, captionpos = b, label={code:psuedotabu}]{codeRelated/Python/psuedo_tabu.py}

The search is given a configuration as a list of connected \textit{Module} objects and an order as a list of \textit{Recipe} objects. In addition it takes a special transport module \textit{transport}, which it used to append configurations and make them physcially possible as described in \cref{ssec:conflicts}. Lastly, it takes the amount of iterations to perform \textit{iter} and the maximum size of short term memory \textit{max\_short\_size}. The search starts by setting up its memory. Short term memory has a fixed size, while long term does not. Dynamic memory is a mapping from configurations to their rating. It keeps record, so that we do not have to recalculate the rating of the same configuration several times. 

Next a call to \textit{get\_init\_configs} generates all initial single line configurations, which can produce the stated order using a minimum amount of modules. This is done by composing the functional dependency graphs of each recipe, which is stored in each \textit{Recipe} object as a dictionary. For every topological sort of the resulting graph, we place down a line of modules accordingly. 

The entire set of initial configurations are evaluated using the \textit{evaluate} function. This function uses  \cref{code:get_best_time} to produce the rating as described in \cref{sec:uppaalpython}. It also updates \textit{dynamic\_memory} to record the rating for the given configuration. Once evaluated, the configurations are added as memory to long term memory. We choose a memory to be defined as an entire configuration. In the actual implementation, configurations are stored as describtive strings. If we ever need to go back to a configuration, we can use one of these memory strings, to set up the configuration again as a list of connected \textit{Module} objects.

Once all initial configurations have been evaluated, we pick the one with the best rating as our frontier. From this point on the actual search occurs for \textit{iter} iterations in the following loop. At each iteration we call \textit{get\_neighbour\_func}. This function returns one of the three functions, which may generate frontier neighbours: \textit{neighbours\_anti\_serialized}, \textit{neighbours\_parallelize} or \textit{neighbours\_swap}.

Using the selected function \textit{func}, we generate a set of neighbours to our frontier. These are all evaluated and the frontier is chosen as the best configuration in the set of neighbours, which are not included in short term memory. If no frontier is found, either no neighbors were generated or short term memory excludes them all, we call \textit{backtack} to find an old configuration in long term memory to use as frontier. Thus, we use long term as a way to reset our search, when in a bad situation. Having now found a new frontier, it is added to short and long term memory. If short term memory is overflowing, we remove its oldest element to make room. 

Once we exit the loop, we return the best configuration found during the search. 